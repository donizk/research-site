---
title: "A deep dive into Data Analytics"
description: ""
date: 2022-03-31T23:39:05-04:00
draft: false
tags: [data-analysis, Python]
---

A useful summary of some research articles I found during my deep dive.

### Survey Article from the literature

 - Citation of the article

 [Review of Leading Data Analytics Tools](https://www.sciencepubco.com/index.php/ijet/article/view/18190/8461)

 - In two or three sentences, discuss the goals of this article.

 The goal of this article is to provide a discussion on data analytics tools that are currently popular in use, with a particulat focus on discussing appropriate uses for each tool (and any limitations that may come with using the tools). Additionally, the article provides discussion on how to select the appropriate tool for their goals.

 - What is /are the general task(s) that all the methods address in the article?

 This article does not discuss methods but rather, it discusses tools. The tools this article discusses include R, Python, RapidMiner, Hadoop, Spark, Tableau, and KNIME. R is a popular tool and some of the highlights of using this tool include the vast number of statistical, analytical, and graphical packages that can be used on data, it can create publication-ready graphics, and user run software, among others. Some limitations of R include that it can only run and process data that is equal to the size of the memory of the machine it is being run on, not easy for the novice to learn, it can't scale properly with larger data sets, and some processes can take days to run. Python is another popular tool, with a rich set of libraries to tackle manipulating and analyzing data. Some limitations of Python include that is more friendly to a programmer than a statistician, lack of commercial support, there's not proper multiprocessor support, and lack of UI development framework. RapidMiner is a popular and newer commercial data analysis tool, which employs machine learning measures and data mining measures to analyze data, GUI based, good for predicitive analysis, can access data through the web and/or the cloud, can run over 1500 different operations, and a nice working environment. Some limitations of RapidMiner include that it is more suitable for users comfortable with databases, limited partitioning abilities for dataset to testing and training sets. Hadoop is an industry-standard tool with a distributed file system, an elastic architecture for large scale computation, can work on multiple different servers at the same time, scalable, cost effective, flexible, and resilient to failure. Limitations to Hadoop include concerns with security, vulnerable nature of the tool, unfit for use with small data sets, and potential stability issues. Spark is a rapid and general mechanism for large scale data processing, analyzing data in real time, good for in-memory computations, and has a strong working set of libraries that are specialized for targeted use. Tableau is a top visualization tool, which is easily accessible, alluring with its million row limit, no coding requirement, easy integration with other data platforms, good user interface, and offers powerful visualizations. Limitations to Tableau include limits on data size, can't connect to R, all data is public, and only some data sources can be read by the tool. KNIME is a tool good for all users employing data mining and machine learning, easy to understand, GUI with a drag and drop nature and thousands of modules with sample code, integrated tools, and/or algorithms to use on data, and is generally more useful for molecular analysis. Some limitations of KNIME include poor visualizations, not suitable for large and/or difficult workflows, can't partition more than 1 data set at a time, limited error measurement methods, no wrapper methods, and preliminary results are not available.

 - What are some of the leading methods introduced and discussed in this paper?

 This article introduces and discusses R, Python, RapidMiner, Hadoop, Spark, Tableau, and KNIME.

 - What is the context of the discussion of these methods? For instance, are there demonstrations in light of some task that they address?

 The context of the discussion of these tools is to highlight what tool is most appropriate for what types of tasks/uses.

 - How can some of these methods be used in your own work? What purpose would the method serve in your work?

These tools are useful for me to learn about as I can begin to formulate an understanding what tool will be most useful in my research. I hope to employ one or more of these tools into my work to help me with my analysis and visualizations.

---

### First article from the literature

 - Citation of the article

[The Data Revolution and Economics](https://www.journals.uchicago.edu/doi/full/10.1086/674019)

 - What is the goal of this article?

 The main goal of this article is to discuss how the emergence of "big data" has impacted and will continue to impact economic policy and research. Specifically, the article looks to discuss how this data can impact economic policy and describe economic activity, trace consequences of different events and/or policies, outline the  challenges in accessing and using the data available, and consider what tools have emerged that may be useful in economic analysis.

 - In about 100 words, please summarize the main goal of the article.

 The article starts with a discussion in the ways that technology has improved and accelerated data collection. In years past, the availability and accessibility to complete data sets that encompassed topics such as economic activity were not as readily useable as data is today. In recent years and with the improvement of technologies and/or processes in capturing data and hosting it on sites such as governmental ones like the census website that hosts demographic, economic, and population data, it has become much more feasible and easier nowadays to find and use data that is collected in real-time, on a larger scale, allowing for more in-depth analyses. Data sets are also more plentiful and common, and naturally as the methods of collecting data are different across what sector the data is being collected for, it is more common nowadays to also come across more data sets that are less structured but offer a higher dimensionality. Another thing that modern technology has done for data and specifically for analyzing economic data is introduce novel variables that would have otherwise not have been captured or considered in prior analyses. The article then goes on to discuss the primary uses of big data for businesses, which includes tracking business processes and outcomes and building predictive models. The authors then discuss how the availability of new data affects economic policy and research by highlighting that governmental administrative data sets like those maintained by the IRS, Social Security Administration, and Medicare/Medicaid are currently underutilized and can be inaccessible to the public, as most of these data sets are under restricted access by researchers looking to use the data to make new discoveries. The author also makes a point of dicussing how governmental administrative data sets are usually independently maintained, instead of having several different types of data merged together into a single set like in many European countries. The next impact big data has on economic policy and research is through the introduction of new measures of private sector economic activity, which was traditionally only captured using survey methods. With the availability of more data, alternative measures of collecting large-scale, and at-times real-time, data on prices, employment, and spending are becoming available, like the consumer spending and employment measures that already exist for many cards and cellular financial applications including Mastercard's SpendingPulse and Mint. Another impact data can have is in improving current government operations and services, as with any business, many decisions are determined by extensive data analytics, and the authors make the point that most private sector companies are more efficient in applying the changes/improvements that are found through data than most public entities like the government, and suggest that as a result, suggest using data analysis to improve governmental services and operations. The last impact of big data the authors discuss is that of using big data to create predictive models to automate business processes or to improve and/or develop new products or services, and they specifically highlight using these for products geared towards consumer protection to avoid decisions they will regret and in the targeting of governmental services to certain audiences. The main challenges present when using big data in economic research include data access, data management and computation, and asking the right questions of the data. The authors end with emphasizing that big data will not substitute the need for "common sense, economic theory, or the need of careful research designs" and that it complements these things, instead. The future of data in economic research and in informing economic policy is bright!

---

### Second article from the literature

 - Citation of the article

 [Emerging practices and perspectives on Big Data analysis in economics: Bigger and better or more of the same?](https://journals.sagepub.com/doi/full/10.1177/2053951714536877?utm_source=summon&utm_medium=discovery-provider)

 - What is the goal of this article?

 The goal of this article is to use interviews conducted with a cross-section of economists representing various concentrations of economics to set up a discussion to examine the general perceived impact of Big Data on economics across different applications.

 - In about 100 words, please summarize the main goal of the article.

 The main goal is to outline the areas Big Data is already being used in the field of economics and to analyze the broader current and potential contributions arising around Big Data. The authors start the article by stating that the biggest issue emerging in economics in relation to the use of Big Data deals with finding new ways (both epistemologically and computationally) to work with the rich data sets being introduced with Big Data. One of the biggest contributions the emergence of Big Data has on the field of economics is in being a valuable and reliable source of information about the social world. Compared to other social sciences, economics has been relatively slow to pick up on experimenting with these new data sets that are being introduced through Big Data and technological advancement. The authors chose to focus on the impact of Big Data as it relates to economics because they feel that the field of economics' position as the intersection between academic and applied knowledge used for business purposes would be a good example of a case study where Big Data can be useful but also have some drawbacks. The main reason for this slow push to start in using the newer data sets is because of strong skepticism from economists, who can have strong theories and methodologies as their foundations, making relying on validity of data and/or believing data is truly representative of an entire population difficult for most to accept. The authors' also provide their own definition for big data, different from the technologically-focused industry-savvy definition, which is "the step change in scale and/or scope of sources of materials and tools for manipulating these available in relation to a given object of interest". The main points the authors highlight in support of the use of Big Data in the field of economics includes the new-found availability and accessibility of data captured in real-time allowing for now-casting (identification of economic trends as they are occurring), improved scale of data which resolves statistical problem of too few observations and potentially creating for a more powerful/accurate analysis, and the inclusion of data that covers aspects of human behavior that were otherwise difficult to observe. The authors also discussed some drawbacks to the use of Big Data in economics that keep economists from adopting it into their work, highlighting specifically the fact that data sets are often largely unstructured and complex as it relates to linkages between variables in the data sets, as well as, presenting the econometric challenge of using statistical techniques appropriate for entire populations' data. The authors also emphasized that economists may be suitable users of Big Data already as most economists' traditional training requires the understanding and knowledge of statistics and some sort of computation, as well as, the fact that there is already a considerable amount of Big Data that is already found within economics like financial transactions. The discussion had in this article suggests that the value of Big Data in economics is largely in creating an impetus for new thinking and in challenging economists to apply theory and methodology to an ever-evolving digital reality.
